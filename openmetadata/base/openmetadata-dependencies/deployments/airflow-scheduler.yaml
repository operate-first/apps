apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  labels:
    app: airflow
    component: scheduler
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      ## multiple schedulers can run concurrently (Airflow 2.0)
      maxSurge: 25%
      maxUnavailable: 0
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app: airflow
        component: scheduler
    spec:
      restartPolicy: Always
      nodeSelector: {}
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - airflow
              topologyKey: kubernetes.io/hostname
      tolerations: []
      serviceAccountName: airflow
      initContainers:
        - name: check-db
          image: quay.io/os-climate/ingestion:0.13.1
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: openmetadata-dependencies-config-envs
            - configMapRef:
                name: openmetadata-dependencies-config-envs
          env:
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-mysql-secrets
                  key: airflow-mysql-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec timeout 60s airflow db check"
          volumeMounts:
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
        - name: wait-for-db-migrations
          image: quay.io/os-climate/ingestion:0.13.1
          imagePullPolicy: IfNotPresent
          envFrom:
            - secretRef:
                name: openmetadata-dependencies-config-envs
            - configMapRef:
                name: openmetadata-dependencies-config-envs
          env:
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-mysql-secrets
                  key: airflow-mysql-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow db check-migrations -t 60"
          volumeMounts:
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
      containers:
        - name: airflow-scheduler
          image: quay.io/os-climate/ingestion:0.13.1
          imagePullPolicy: IfNotPresent
          resources: {}
          envFrom:
            - secretRef:
                name: openmetadata-dependencies-config-envs
            - configMapRef:
                name: openmetadata-dependencies-config-envs
          env:
            - name: DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: airflow-mysql-secrets
                  key: airflow-mysql-password
            - name: CONNECTION_CHECK_MAX_COUNT
              value: "0"
          command:
            - "/usr/bin/dumb-init"
            - "--"
            - "/entrypoint"
          args:
            - "bash"
            - "-c"
            - "exec airflow scheduler -n -1"
          livenessProbe:
            initialDelaySeconds: 10
            periodSeconds: 30
            failureThreshold: 5
            timeoutSeconds: 60
            exec:
              command:
                - "/usr/bin/dumb-init"
                - "--"
                - "/entrypoint"
                - "python"
                - "-Wignore"
                - "-c"
                - |
                  import os
                  import sys

                  # suppress logs triggered from importing airflow packages
                  os.environ["AIRFLOW__LOGGING__LOGGING_LEVEL"] = "ERROR"

                  from airflow.jobs.scheduler_job import SchedulerJob
                  from airflow.utils.db import create_session
                  from airflow.utils.net import get_hostname

                  with create_session() as session:
                      ########################
                      # heartbeat check
                      ########################
                      # ensure the SchedulerJob with most recent heartbeat for this `hostname` is alive
                      hostname = get_hostname()
                      scheduler_job = session \
                          .query(SchedulerJob) \
                          .filter_by(hostname=hostname) \
                          .order_by(SchedulerJob.latest_heartbeat.desc()) \
                          .limit(1) \
                          .first()
                      if (scheduler_job is not None) and scheduler_job.is_alive():
                          pass
                      else:
                          sys.exit(f"The SchedulerJob (id={scheduler_job.id}) for hostname '{hostname}' is not alive")
          volumeMounts:
            - name: dags-data
              mountPath: /opt/airflow/dags
            - name: logs-data
              mountPath: /opt/airflow/logs
            - name: pod-template
              mountPath: /opt/airflow/pod_templates/pod_template.yaml
              subPath: pod_template.yaml
              readOnly: true
      volumes:
        - name: dags-data
          persistentVolumeClaim:
            claimName: airflow-dags
        - name: logs-data
          persistentVolumeClaim:
            claimName: airflow-logs
        - name: pod-template
          configMap:
            name: airflow-pod-template
