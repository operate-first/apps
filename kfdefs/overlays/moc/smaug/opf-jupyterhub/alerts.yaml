apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: jupyterhub-alerts
spec:
  groups:
    - name: jupyterhub
      rules:
        - alert: PVCFillingUp
          annotations:
            description: >-
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value |
              humanizePercentage }} free.
            summary: PersistentVolume is filling up.
            # Tag the user who owns the PVC
            user: >-
              @{{ reReplaceAll "^jupyterhub-nb-|-pvc$" ""
              $labels.persistentvolumeclaim }}
            # Link the runbook with instructions to fix this issue
            runbook: >-
              https://github.com/operate-first/apps/blob/master/docs/content/odh/jupyterhub/runbook.md#insufficient-disk-space-for-notebook-pod
          # Alert when 90% of PVC is full
          expr: >-
            kubelet_volume_stats_available_bytes{namespace="opf-jupyterhub"}/kubelet_volume_stats_capacity_bytes{namespace="opf-jupyterhub"}
            < 0.1
          labels:
            severity: critical
    - name: SLOs - JupyterHub
      rules:
        # The haproxy errors SLO for JupyterHub was disabled due to misleading return codes that are being generated by JupyterHub
        # More information can be found in https://issues.redhat.com/browse/RHODS-2101
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[1d]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[1d]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate1d
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[1h]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[1h]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate1h
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[2h]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[2h]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate2h
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[30m]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[30m]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate30m
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[3d]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[3d]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate3d
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[5m]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[5m]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate5m
        # - expr: |
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[6h]))
        #     /
        #     sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[6h]))
        #   labels:
        #     route: jupyterhub
        #   record: haproxy_backend_http_responses_total:burnrate6h

        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[1d])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate1d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[1h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate1h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[2h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate2h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[30m])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate30m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[3d])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate3d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[5m])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate5m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[6h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate6h
    - name: SLOs-haproxy_backend_http_responses_total
      rules:
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate5m{route=~"rhods-dashboard"}) by (route) > (14.40 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate1h{route=~"rhods-dashboard"}) by (route) > (14.40 * (1-0.98000))
          for: 2m
          labels:
            severity: critical
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate30m{route=~"rhods-dashboard"}) by (route) > (6.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate6h{route=~"rhods-dashboard"}) by (route) > (6.00 * (1-0.98000))
          for: 15m
          labels:
            severity: critical
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate2h{route=~"rhods-dashboard"}) by (route) > (3.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate1d{route=~"rhods-dashboard"}) by (route) > (3.00 * (1-0.98000))
          for: 1h
          labels:
            severity: warning
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate6h{route=~"rhods-dashboard"}) by (route) > (1.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate3d{route=~"rhods-dashboard"}) by (route) > (1.00 * (1-0.98000))
          for: 3h
          labels:
            severity: warning
    - name: SLOs-probe_success
      rules:
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(probe_success:burnrate5m{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (14.40 * (1-0.98000))
            and
            sum(probe_success:burnrate1h{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (14.40 * (1-0.98000))
          for: 2m
          labels:
            severity: critical
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(probe_success:burnrate30m{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (6.00 * (1-0.98000))
            and
            sum(probe_success:burnrate6h{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (6.00 * (1-0.98000))
          for: 15m
          labels:
            severity: critical
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(probe_success:burnrate2h{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (3.00 * (1-0.98000))
            and
            sum(probe_success:burnrate1d{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (3.00 * (1-0.98000))
          for: 1h
          labels:
            severity: warning
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }}).'
            summary: RHODS Route Error Burn Rate
          expr: |
            sum(probe_success:burnrate6h{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (1.00 * (1-0.98000))
            and
            sum(probe_success:burnrate3d{instance=~"jupyterhub|rhods-dashboard|jupyterhub-db|traefik"}) by (instance) > (1.00 * (1-0.98000))
          for: 3h
          labels:
            severity: warning
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus
  namespace: redhat-ods-monitoring
data:
  prometheus.yml: |
    rule_files:
      - '*.rules'
    global:
      scrape_interval:     10s
      evaluation_interval: 10s

    scrape_configs:
    - job_name: 'Federate Prometheus'
      scrape_interval: 30s
      scheme: https
      tls_config:
        server_name: <federate_target>
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token: "<prom_bearer_token>"
      honor_labels: true
      metrics_path: '/federate'

      params:
        'match[]':
          - '{__name__= "haproxy_backend_http_responses_total"}'
          - '{__name__= "controller_runtime_reconcile_total"}'
          - '{__name__= "container_cpu_usage_seconds_total"}'
          - '{__name__= "container_memory_rss"}'
          - '{__name__= "kubelet_volume_stats_used_bytes"}'
          - '{__name__= "kubelet_volume_stats_capacity_bytes"}'
          - '{__name__= "kube_pod_container_status_waiting_reason"}'
          - '{__name__= "kube_pod_container_status_restarts_total"}'
          - '{__name__= "kube_pod_container_status_terminated_reason"}'
          - '{__name__= "openshift_build_status_phase_total"}'

      static_configs:
        - targets:
          - "<federate_target>"
    - job_name: 'user_facing_endpoints_status'
      scrape_interval: 10s
      metrics_path: /probe
      scheme: https
      tls_config:
        insecure_skip_verify: true
      params:
        module: [http_2xx]
      authorization:
        credentials_file: /run/secrets/kubernetes.io/serviceaccount/token
      static_configs:
      - targets:
        - <jupyterhub_host>
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: jupyterhub-opf-jupyterhub.apps.smaug.na.operate-first.cloud/hub/metrics
