kind: ConfigMap
apiVersion: v1
metadata:
  name: ray-cluster-template
data:
  rayClusterResourceTemplate: |
    kind: RayCluster
    apiVersion: cluster.ray.io/v1
    metadata:
      name: 'ray-cluster-{{ user }}'
      labels:
        # allows me to return name of service that Ray operator creates
        odh-ray-cluster-service: 'ray-cluster-{{ user }}-ray-head'
    spec:
      # we can parameterize this when we fix the JH launcher json/jinja bug
      maxWorkers: 5
      # The autoscaler will scale up the cluster faster with higher upscaling speed.
      # E.g., if the task requires adding more nodes then autoscaler will gradually
      # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
      # This number should be > 0.
      upscalingSpeed: 1.0
      # If a node is idle for this many minutes, it will be removed.
      idleTimeoutMinutes: 30
      # Specify the pod type for the ray head node (as configured below).
      headPodType: head-node
      # Specify the allowed pod types for this ray cluster and the resources they provide.
      podTypes:
      - name: head-node
        podConfig:
          apiVersion: v1
          kind: Pod
          metadata:
            generateName: 'ray-cluster-{{ user }}-head-'
          spec:
            restartPolicy: Never
            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
            containers:
            - name: ray-node
              imagePullPolicy: Always
              image: '{{ ray_image }}'
              # Do not change this command - it keeps the pod alive until it is explicitly killed.
              command: ["/bin/bash", "-c", "--"]
              args: ['trap : TERM INT; sleep infinity & wait;']
              # This volume allocates shared memory for Ray to use for plasma
              env:
              # defining HOME is part of a workaround for:
              # https://github.com/ray-project/ray/issues/14155
              - name: HOME
                value: '/home'
              volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              resources:
                requests:
                  cpu: '{{ cpu_request }}'
                  memory: '{{ memory_request}}'
                limits:
                  cpu: '{{ cpu_limit }}'
                  # The maximum memory that this pod is allowed to use. The
                  # limit will be detected by ray and split to use 10% for
                  # redis, 30% for the shared memory object store, and the
                  # rest for application memory. If this limit is not set and
                  # the object store size is not set manually, ray will
                  # allocate a very large object store in each pod that may
                  # cause problems for other pods.
                  memory: '{{ memory_limit }}'
      - name: worker-nodes
        # we can parameterize this when we fix the JH launcher json/jinja bug
        minWorkers: 0
        maxWorkers: 5
        podConfig:
          apiVersion: v1
          kind: Pod
          metadata:
            # Automatically generates a name for the pod with this prefix.
            generateName: 'ray-cluster-{{ user }}-worker-'
          spec:
            restartPolicy: Never
            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
            containers:
            - name: ray-node
              imagePullPolicy: Always
              image: '{{ ray_image }}'
              command: ["/bin/bash", "-c", "--"]
              args: ["trap : TERM INT; sleep infinity & wait;"]
              env:
              - name: HOME
                value: '/home'
              volumeMounts:
              - mountPath: /dev/shm
                name: dshm
              resources:
                requests:
                  cpu: '{{ cpu_request }}'
                  memory: '{{ memory_request}}'
                limits:
                  cpu: '{{ cpu_limit }}'
                  memory: '{{ memory_limit }}'
      # Commands to start Ray on the head node. You don't need to change this.
      # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
      headStartRayCommands:
          - cd /home/ray; pipenv run ray stop
          - ulimit -n 65536; cd /home/ray; pipenv run ray start --head --no-monitor --port=6379 --object-manager-port=8076 --dashboard-host=0.0.0.0
      # Commands to start Ray on worker nodes. You don't need to change this.
      workerStartRayCommands:
          - cd /home/ray; pipenv run ray stop
          - ulimit -n 65536; cd /home/ray; pipenv run ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
  rayDashboardRouteTemplate: |
    kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: 'ray-dashboard-{{ user }}'
      labels:
        # allows me to return name of service that Ray operator creates
        odh-ray-cluster-service: 'ray-cluster-{{ user }}-ray-head'
    spec:
      to:
        kind: Service
        name: 'ray-cluster-{{ user }}-ray-head'
      port:
        targetPort: dashboard
